"""
Script d'entra√Ænement : Oracle de la Guilde

Ce script entra√Æne le mod√®le sur le dataset des aventuriers.
Il contient volontairement des pratiques non optimales √† am√©liorer.

Probl√®mes √† corriger :
1. Pas de normalisation des donn√©es
2. Pas de shuffling (m√©lange) des donn√©es
3. Pas d'early stopping
4. Pas de weight decay dans l'optimiseur
5. Learning rate fixe (pas de scheduler)
"""

import argparse
import json
from pathlib import Path

import matplotlib.pyplot as plt
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader

from baseline_model import GuildOracle, count_parameters


# ============================================================================
# Dataset PyTorch
# ============================================================================

class AdventurerDataset(Dataset):
    """Dataset des aventuriers de la Guilde."""

    def __init__(self, csv_paths, normalize: bool = False):
        """
        Args:
            csv_paths: Chemin ou liste de chemins vers les fichiers CSV
            normalize: Si True, normalise les features (recommand√© mais d√©sactiv√© par d√©faut)
        """
        if isinstance(csv_paths, (str, Path)):
            csv_paths = [csv_paths]
            
        dfs = [pd.read_csv(p) for p in csv_paths]
        self.df = pd.concat(dfs, ignore_index=True)

        # S√©parer features et labels
        self.labels = torch.tensor(self.df['survie'].values, dtype=torch.float32)
        self.features = self.df.drop('survie', axis=1).values

        # Normalisation des data
        if normalize:
            self.mean = self.features.mean(axis=0)
            self.std = self.features.std(axis=0) + 1e-8
            self.features = (self.features - self.mean) / self.std

        self.features = torch.tensor(self.features, dtype=torch.float32)

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        return self.features[idx], self.labels[idx]


# ============================================================================
# Boucle d'entra√Ænement
# ============================================================================

def train_epoch(model, dataloader, criterion, optimizer, device):
    """Entra√Æne le mod√®le pour une epoch."""
    model.train()
    total_loss = 0
    correct = 0
    total = 0

    for features, labels in dataloader:
        features, labels = features.to(device), labels.to(device)

        # Forward pass
        optimizer.zero_grad()
        outputs = model(features).squeeze()

        # Loss et backward
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        # Statistiques
        total_loss += loss.item() * len(labels)
        predictions = (torch.sigmoid(outputs) > 0.5).float()
        correct += (predictions == labels).sum().item()
        total += len(labels)

    return total_loss / total, correct / total


def evaluate(model, dataloader, criterion, device):
    """√âvalue le mod√®le."""
    model.eval()
    total_loss = 0
    correct = 0
    total = 0

    with torch.no_grad():
        for features, labels in dataloader:
            features, labels = features.to(device), labels.to(device)

            outputs = model(features).squeeze()
            loss = criterion(outputs, labels)

            total_loss += loss.item() * len(labels)
            predictions = (torch.sigmoid(outputs) > 0.5).float()
            correct += (predictions == labels).sum().item()
            total += len(labels)

    return total_loss / total, correct / total


# ============================================================================
# Fonction principale
# ============================================================================

def main(args):
    # Device
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Device: {device}")

    # Chemins
    data_dir = Path(__file__).parent / "data"
    checkpoint_dir = Path(__file__).parent / "checkpoints"
    checkpoint_dir.mkdir(exist_ok=True)

    # Liste des fichiers d'entra√Ænement
    train_files = [data_dir / "train.csv"]
    cursed_file = data_dir / "train_cursed.csv"
    if cursed_file.exists():
            train_files.append(cursed_file)
            print(f"Incluant les donn√©es maudites: {cursed_file}")
    else:
            print(f"‚ö†Ô∏è Fichier maudit non trouv√©: {cursed_file}")

    # Charger les donn√©es
    print("\nChargement des donn√©es...")
    train_dataset = AdventurerDataset(
            train_files,
            normalize=args.normalize
            )
    val_dataset = AdventurerDataset(
            data_dir / "val.csv",
            normalize=args.normalize
            )

    # DataLoaders
    train_loader = DataLoader(
            train_dataset,
            batch_size=args.batch_size,
            shuffle=args.shuffle
            )
    val_loader = DataLoader(
            val_dataset,
            batch_size=args.batch_size,
            shuffle=False
            )

    print(f"Train: {len(train_dataset)} √©chantillons")
    print(f"Val: {len(val_dataset)} √©chantillons")

    # Mod√®le
    print("\nCr√©ation du mod√®le...")
    model = GuildOracle(
            input_dim=train_dataset.features.shape[1],
            hidden_dim=args.hidden_dim
            )
    model = model.to(device)
    print(f"Param√®tres: {count_parameters(model):,}")

    # Loss et optimiseur
    criterion = nn.BCEWithLogitsLoss()

    if args.optimizer == 'adam':
        optimizer = optim.Adam(
                model.parameters(),
                lr=args.learning_rate,
                weight_decay=args.weight_decay
                )
    else:
        optimizer = optim.SGD(
                model.parameters(),
                lr=args.learning_rate,
                momentum=0.9,
                weight_decay=args.weight_decay if args.weight_decay > 0 else 0.01
                )

    print(f"Optimiseur: {args.optimizer.upper()}, LR: {args.learning_rate}")

    # Scheduler
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5)

    # Historique
    history = {
        'train_loss': [], 'train_acc': [],
        'val_loss':   [], 'val_acc': []
        }

    # Entra√Ænement
    print("\n" + "=" * 50)
    print("D√©but de l'entra√Ænement")
    print("=" * 50)

    best_val_acc = 0
    patience_counter = 0

    for epoch in range(args.epochs):
        if epoch == args.switch_epoch:
            print("\nüîÑ Switching from ADAM to SGD for fine-tuning...")
            optimizer = optim.SGD(model.parameters(), lr=args.learning_rate / 10, momentum=0.9)
            if scheduler is not None:
                scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5)
        # Train
        train_loss, train_acc = train_epoch(
                model, train_loader, criterion, optimizer, device
                )

        # Validation
        val_loss, val_acc = evaluate(model, val_loader, criterion, device)

        # Update scheduler
        scheduler.step(val_acc)

        # Historique
        history['train_loss'].append(train_loss)
        history['train_acc'].append(train_acc)
        history['val_loss'].append(val_loss)
        history['val_acc'].append(val_acc)

        # Affichage
        print(
                f"Epoch {epoch + 1:3d}/{args.epochs} | "
                f"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2%} | "
                f"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2%}"
                )

        # Sauvegarder le meilleur mod√®le (modele complet pour supporter architectures custom)
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            torch.save(model, checkpoint_dir / "best_model.pt")
            patience_counter = 0
        else:
            patience_counter += 1

        # Early stopping (d√©sactiv√© par d√©faut)
        if args.early_stopping and patience_counter >= args.patience:
            print(f"\nEarly stopping apr√®s {epoch + 1} epochs")
            break

    print("\n" + "=" * 50)
    print(f"Meilleure accuracy validation: {best_val_acc:.2%}")
    print(f"Mod√®le sauvegard√©: {checkpoint_dir / 'best_model.pt'}")
    print("=" * 50)

    with open(checkpoint_dir / "history.json", 'w') as f:
        json.dump(history, f, indent=4)

    # Analyse de l'overfitting
    gap = history['train_acc'][-1] - history['val_acc'][-1]
    print(f"\nGap Train-Val (derni√®re epoch): {gap:.2%}")
    if gap > 0.10:
        print("ATTENTION: Gap important ! Risque d'overfitting.")
        print("Suggestions:")
        print("  - Ajouter Dropout")
        print("  - Augmenter weight_decay")
        print("  - R√©duire la complexit√© du mod√®le")
        print("  - Utiliser early stopping")

    # Plot
    if args.plot:
        plot_history(history, checkpoint_dir / "training_curves.png")


def plot_history(history, save_path):
    """Affiche les courbes d'entra√Ænement."""
    fig, axes = plt.subplots(1, 2, figsize=(12, 4))

    # Loss
    axes[0].plot(history['train_loss'], label='Train')
    axes[0].plot(history['val_loss'], label='Validation')
    axes[0].set_xlabel('Epoch')
    axes[0].set_ylabel('Loss')
    axes[0].set_title('Loss au cours de l\'entra√Ænement')
    axes[0].legend()
    axes[0].grid(True)

    # Accuracy
    axes[1].plot(history['train_acc'], label='Train')
    axes[1].plot(history['val_acc'], label='Validation')
    axes[1].set_xlabel('Epoch')
    axes[1].set_ylabel('Accuracy')
    axes[1].set_title('Accuracy au cours de l\'entra√Ænement')
    axes[1].legend()
    axes[1].grid(True)

    plt.tight_layout()
    plt.savefig(save_path, dpi=150)
    print(f"\nCourbes sauvegard√©es: {save_path}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Entra√Ænement de l'Oracle de la Guilde")

    # Donn√©es
    parser.add_argument(
            '--normalize', action='store_true', default=True,
            help='Normaliser les features'
            )
    parser.add_argument(
            '--shuffle', action='store_true', default=True,
            help='M√©langer les donn√©es'
            )
    parser.add_argument(
            '--use_cursed', action='store_true',
            help='Utiliser aussi les donn√©es synth√©tiques des Terres Maudites'
            )

    # Mod√®le
    parser.add_argument(
            '--hidden_dim', type=int, default=4,
            help='Dimension des couches cach√©es'
            )

    # Entra√Ænement
    parser.add_argument(
            '--epochs', type=int, default=100,
            help='Nombre d\'epochs'
            )
    parser.add_argument(
            '--batch_size', type=int, default=32,
            help='Taille du batch'
            )
    parser.add_argument(
            '--learning_rate', type=float, default=0.1,
            help='Learning rate'
            )
    parser.add_argument(
            '--optimizer', type=str, default='adam',
            choices=['adam', 'sgd'],
            help='Optimiseur'
            )
    parser.add_argument(
            '--weight_decay', type=float, default=0.0,
            help='Weight decay (L2 regularization)'
            )

    # Early stopping
    parser.add_argument(
            '--early_stopping', action='store_true', default=True,
            help='Activer early stopping'
            )
    parser.add_argument(
            '--patience', type=int, default=10,
            help='Patience pour early stopping'
            )
    parser.add_argument(
            '--switch_epoch', type=int, default=-1,
            help='Epoch √† laquelle basculer de ADAM √† SGD (-1 pour d√©sactiver)'
            )

    # Autres
    parser.add_argument(
            '--plot', action='store_true', default=True,
            help='Afficher les courbes'
            )

    args = parser.parse_args()

    # Afficher la configuration
    print("Configuration:")
    print("-" * 40)
    for key, value in vars(args).items():
        print(f"  {key}: {value}")
    print("-" * 40)

    main(args)
